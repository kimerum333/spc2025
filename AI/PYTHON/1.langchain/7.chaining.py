from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate
from langchain_core.runnables import RunnableLambda

from langchain_openai import OpenAI

print("ğŸ”§ Loading environment variables...")
load_dotenv()

# 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬ ì—†ìŒ
chat_prompt1 = PromptTemplate(
    input_variables = ["product"],
    template = "ë„ˆëŠ” íšŒì‚¬ ì´ë¦„ì„ ì „ë¬¸ì ìœ¼ë¡œ ì§“ëŠ” ì‘ëª…ê°€ì•¼. ë‹¤ìŒ ìƒí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ê°–ëŠ” íšŒì‚¬ëª…ì„ ì§€ì–´ì¤˜. ìƒí’ˆëª…:{product}"
)
chat_prompt2 = PromptTemplate(
    input_variables = ["company_name"],
    template = "ì´ íšŒì‚¬ë¥¼ ì˜ ì†Œê°œí•  ìˆ˜ ìˆëŠ” ìŠ¬ë¡œê±´, ë˜ëŠ” ìºì¹˜í”„ë ˆì´ì¦ˆë¥¼ ë§Œë“¤ì–´ ì¤˜. íšŒì‚¬ëª…:{company_name}"
)

llm = OpenAI(model = "gpt-3.5-turbo-instruct",temperature=0.9)
# llm = ChatOpenAI(model = "gpt-4o",temperature=0.9)

chain1 = (
    chat_prompt1 | llm | RunnableLambda(lambda x: {"company_name":x.strip()}) |
    chat_prompt2 | llm | RunnableLambda(lambda x: {"catch_phrase":x.strip()})
)
response1 = chain1.invoke({"product":"ì†Œìš¸ë¼ì´í¬ ì•¡ì…˜RPG"})["catch_phrase"]
print(response1)