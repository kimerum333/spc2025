from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.runnables import RunnableLambda

from langchain_openai import ChatOpenAI

print("ğŸ”§ Loading environment variables...")
load_dotenv()

llm = ChatOpenAI(model = "gpt-3.5-turbo",temperature=0.5)

# 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬ ì—†ìŒ
chat_prompt1 = ChatPromptTemplate.from_template(
    "ë„ˆëŠ” ìš”ë¦¬ì‚¬ì•¼. ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë‹µë³€í•´ì¤˜. : {input}"
)
chain1 = chat_prompt1 | llm | RunnableLambda(lambda x: {"response" : x.content})

response = chain1.invoke({"input" : "ê¹€ì¹˜ë¥¼ ë§Œë“œëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì„ ì•Œë ¤ì¤˜"})["response"]
print(response)


# 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ë¦¬í•¨
system_template = """ë„ˆëŠ” ì „ë¬¸ ë²ˆì—­ê°€ì•¼. ë‹¤ìŒ ê¸€ì„ ë³´ê³  {input_language} ë¡œë¶€í„°
{output_language}ë¡œ ë²ˆì—­ì„ í•´ì•¼ í•´.
 """

system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)
human_message_prompt = HumanMessagePromptTemplate.from_template("{text}")

chat_prompt2 = ChatPromptTemplate.from_messages(
    [system_message_prompt,human_message_prompt]
)

chain2 = chat_prompt2 | llm | RunnableLambda(lambda x: {"response" : x.content})
response2 = chain2.invoke({"input_language":"ì˜ì–´","output_language" : "í•œêµ­ì–´", "text" : "Hello Donald Trump! I am Han, Minister of magic in Korea"})
print(response2)

# 3. í›„ì²˜ë¦¬ íŒŒì„œ ì‚¬ìš©
from langchain_core.output_parsers import CommaSeparatedListOutputParser
chain3 = chat_prompt2 | llm | CommaSeparatedListOutputParser()
response3 = chain3.invoke({"input_language":"ì˜ì–´","output_language" : "í•œêµ­ì–´", "text" : "Hello Donald Trump! I am Han, Minister of magic in Korea"})
print(response3)
