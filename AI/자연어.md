# 자연어 처리(NLP)
- Natural Language Processing
- Natural Language Understanding

# 절차
## 전처리
- 단어들을 어근 형태로 축소 불필요한 단어 제거
- 토큰화
- 벡터화

## NLP는 뭘 하는가
- 감정분석
- 자동요약 
- 주제별분류
- 유사성분석

## 아키텍처
- 토큰화
- 구문 분석
- 등등..

## NER named Entity Recognition
- male - female
- verb tense (walking - walked, swimming - swam)
- country - capital ()
- => 단어들을 수천억 차원의 다양한 관점으로 나열.

## 자연어 처리 고전 모델
- RNN (Recurrent Neural Network) : 문장을 한번에 하나씩 순차적으로 처리
- LSTM
- GRU

## 트랜스포머
- Attention is all you need
- self attention 기반.
- 문장 내 중요한 단어를 스스로 학습하고 계산.

### 입력 임베딩
- 입력 텍스트를 토큰화 (어근, 단어별)
- 임베딩 : 토큰 ID를 고정된 크기의 벡터로 변환. 벡터는 단어의 의미와 문맥을 의미
- 예시: '물' 자연 차원에서는 관계가 높다. '고체'차원과는 관계가 적다 '온도'차원과는 영향도가 있다. '감정' 차원에서는 영향도 없고, '단단함'차원에서는 영향도가 음수.
- 문장 내의 단어들이 이 문맥에서는 어떤 의미인가를 이해하기 위해 이것을 다른 문장 내의 단어들과 전부 비교해서 관계성을 파악하고자 연산!!

## 🌐 트랜스포머 작동 절차 정리

### 1. 입력 텍스트 토큰화
- 문장을 subword 단위로 분리하고 각 토큰을 ID로 변환  
  예: `"나는 학교에 간다"` → `["나", "##는", "학", "##교", "##에", "간", "##다"]`

---

### 2. 입력 임베딩 + 포지셔널 인코딩
- 토큰 ID를 고정된 크기의 의미 벡터로 변환 (임베딩)
- 단어의 순서를 고려할 수 있도록 위치 정보를 더함 (포지셔널 인코딩)

---

### 3. 인코더로 문맥 파악
- **Self-Attention**을 통해 문장 내 모든 단어쌍의 관계(유사도)를 계산
- 여러 층의 인코더 블록을 거쳐 문맥 정보를 풍부하게 반영

---

### 4. 디코더로 문장 생성
- 시작 토큰 `<s>`부터 시작해 한 토큰씩 생성
- 디코더 내부 구성:
  - 이전에 생성한 토큰들을 입력으로 사용
  - 인코더 출력과 **인코더-디코더 어텐션**으로 전체 문맥을 참고
  - **FeedForward 신경망**으로 다음 토큰 예측

---

### 5. 출력 확률 분포 → 최종 문장 생성
- 디코더 출력은 단어별 확률 벡터 (Softmax 결과)
- **Greedy Search**, **Beam Search**, **Temperature Sampling** 등의 방식으로 다음 토큰 선택
- 선택된 토큰들을 이어 붙여 최종 문장 완성

--- 

# '너는 ㅁㅁ야'. ㅅㅅ하게 도와줘.
- '너는 ㅁㅁ다'는 **초기 역할 세팅(prompt initialization)**이다.
- GPT는 **문맥을 해당 문장부터 읽어가기 때문에**,  
  초반에 역할을 명확히 설정해주면 **연관성 있는 응답을 유도**하기 쉬워진다.
- 이는 **시스템 프롬프트 또는 인스트럭션 튜닝의 기초**에 해당한다.

## 🛠️ 프롬프트 엔지니어링(Prompt Engineering) 예시

> GPT와 같은 LLM에게 원하는 응답을 이끌어내기 위해  
> 입력 문장을 구조적으로 설계하는 기법

---

### ✅ 대표적인 프롬프트 엔지니어링 기법

| 방식 | 설명 | 예시 |
|------|------|------|
| **역할 부여** | 모델에게 특정 역할을 부여해 톤과 관점을 정함 | `너는 변호사야.` |
| **구체적 요청** | 모호한 지시 대신 구체적인 형식, 스타일을 요구 | `간결하고 항목별로 요약해줘.` |
| **예시 제시** | 원하는 출력 패턴을 예시로 보여줌 | `Q: 질문 / A: 답변` |
| **제약 조건** | 형식, 길이, 문체 등의 제한을 명시함 | `JSON 형식으로 응답해.` |
| **Few-shot 학습** | 여러 문답 예시를 먼저 제시해 따라 하게 유도 | `예시 3~5개를 먼저 보여주고 질문` |

---

### 💡 핵심 팁
- GPT는 "질문을 어떻게 하느냐"에 **매우 민감**하게 반응함.
- `"너는 ㅁㅁ야. ㅅㅅ하게 도와줘."` 구조는 가장 기본적이면서도 강력한 설계법임.
